{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfd61edc",
   "metadata": {},
   "source": [
    "# Scraping Data from Yelp on the Bay Area Tri-Cities\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd166d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "# necessary imports\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5182551",
   "metadata": {},
   "source": [
    "## Inspection\n",
    "\n",
    "First, I begin by taking an initial look at the contents of the `html` file of my Yelp search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9cf9be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en-US\" prefix=\"og: http://ogp.me/ns#\" style=\"margin: 0;padding: 0; border\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.yelp.com/search?find_desc=Restaurants&find_loc=Newark%2C+CA&start=0\"\n",
    "html = requests.get(url)\n",
    "\n",
    "print(html.text[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c726edab",
   "metadata": {},
   "source": [
    "From using Google Chrome developer tools, it appears that the following URL returns a JSON response:\n",
    "\n",
    "https://www.yelp.com/search/snippet?find_desc=Restaurants&find_loc=Newark%2C+CA&start=0\n",
    "\n",
    "From experimenting with the URL, it also appears that I can get the next page of search results by incrementing the `start` variable in the URL by $10$, so if I want the second page of results, I would change it to `set=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d61d466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pageTitle': 'Top 10 Best Restaurants near Newark\n"
     ]
    }
   ],
   "source": [
    "# The API endpoint\n",
    "url = \"https://www.yelp.com/search/snippet?find_desc=Restaurants&find_loc=Newark,+CA&start=0\"\n",
    "\n",
    "# A GET request to the API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Print the response\n",
    "response_json = response.json()\n",
    "print(str(response_json)[:50]) # converted json dict to String for smaller print statements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59344bee",
   "metadata": {},
   "source": [
    "From the looks of the printed result, it appears that the content (a JSON file) matches that of the JSON response link. To understand the file better, I moved the contents into an [online JSON formatter tool](https://jsonformatter.org/). And upon further exploration by searching for `bizId` (most likely the primary key of a restaurant), I determine that I need to index into the list with the keys `searchPageProps` and `mainContentComponentsListProps` to access my desired level of granularity, which is the information on individual businesses. The following two pictures show how I came to that conclusion:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddef0347",
   "metadata": {},
   "source": [
    "!['searchPageProps_img'](online_json_img_1.png)\n",
    "!['mainContentComponentsListProps_img'](online_json_img_2.png)\n",
    "\n",
    "And upon using those keys, I observe the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e22ac44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'bizId': 'CcbFduunKlsrnW71-PH0ZA', 'sea\n",
      "{'bizId': '1y4juqtkSJ9DPPZyRMi-xA', 'sea\n",
      "{'bizId': 'purj1aiUzDi0I__qLOaNRg', 'sea\n",
      "{'bizId': '9Shb0yRis5NEQ5xIGG0FcA', 'sea\n",
      "{'bizId': 'mog4wAikb1EnzwWSnKt-BQ', 'sea\n",
      "{'bizId': '_k7U8HEAsBWScymSKWFAfQ', 'sea\n",
      "{'bizId': 'Ch5JHbQ9KKDeRfZXRJVo2g', 'sea\n",
      "{'bizId': 'JvSvKpVFQ13VM28n7y-BOw', 'sea\n",
      "{'bizId': 'i_GuYJ_1hmW-lHeQ4GmOiw', 'sea\n",
      "{'bizId': 'UhMdgcDu6zRADR43uhyfYg', 'sea\n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n",
      "{'searchResultLayoutType': 'separator', \n"
     ]
    }
   ],
   "source": [
    "results = response.json()['searchPageProps']['mainContentComponentsListProps']\n",
    "\n",
    "for result in results:\n",
    "    print(str(result)[:40]) # converted json dict to String for smaller print statements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95f868a8",
   "metadata": {},
   "source": [
    "This output meant that were still some filtering needed. Luckily, it appears that there are **10** `bizId` fields, which is more than likely the ten businesses from the search result. By investigating the contents further in the online json formatter tool, it appears that the field `searchResultLayoutType` is shared by all of the elements but the ten businesses in question can be differentiated with the filter `searchResultLayoutType == \"iaResult\"`.\n",
    "\n",
    "!['searchResultLayoutType_separator_img'](online_json_img_3.png)\n",
    "!['searchResultLayoutType_iaresult_img'](online_json_img_4.png)\n",
    "\n",
    "In addition, I came to realize that even if a city was specified in a Yelp inquiry, it's not necessarily the case that a restaurant is located in that city. To account for that, the `alias` key can be used, which returns a String containing the restaurant name and the city it is located in (ie. mcdonalds-newark).\n",
    "\n",
    "With these two filters in mind, I proceed to perform the search on the city of Newark with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0da7d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:         Pocha K\n",
      "alias:        pocha-k-newark\n",
      "correct_city: 1\n",
      "review_count: 54\n",
      "rating:       4.5 \n",
      "\n",
      "name:         Mingkee Deli\n",
      "alias:        mingkee-deli-newark\n",
      "correct_city: 1\n",
      "review_count: 16\n",
      "rating:       3.5 \n",
      "\n",
      "name:         四姐 Pan Fried Dumplings\n",
      "alias:        四姐-pan-fried-dumplings-newark-2\n",
      "correct_city: 1\n",
      "review_count: 612\n",
      "rating:       4.5 \n",
      "\n",
      "name:         Duobao BBQ & Dumplings\n",
      "alias:        duobao-bbq-and-dumplings-newark\n",
      "correct_city: 1\n",
      "review_count: 42\n",
      "rating:       4.5 \n",
      "\n",
      "name:         Tamper Room\n",
      "alias:        tamper-room-fremont\n",
      "correct_city: 0\n",
      "review_count: 37\n",
      "rating:       5.0 \n",
      "\n",
      "name:         Mingala Restaurant\n",
      "alias:        mingala-restaurant-newark\n",
      "correct_city: 1\n",
      "review_count: 83\n",
      "rating:       4.5 \n",
      "\n",
      "name:         Always Cool BBQ\n",
      "alias:        always-cool-bbq-fremont-2\n",
      "correct_city: 0\n",
      "review_count: 153\n",
      "rating:       4.5 \n",
      "\n",
      "name:         Oyama BBQ\n",
      "alias:        oyama-bbq-newark-2\n",
      "correct_city: 1\n",
      "review_count: 21\n",
      "rating:       4.5 \n",
      "\n",
      "name:         Lazy Dog Restaurant & Bar\n",
      "alias:        lazy-dog-restaurant-and-bar-newark-2\n",
      "correct_city: 1\n",
      "review_count: 1617\n",
      "rating:       4.0 \n",
      "\n",
      "name:         Billy Roys Burger\n",
      "alias:        billy-roys-burger-fremont-4\n",
      "correct_city: 0\n",
      "review_count: 282\n",
      "rating:       4.5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    if result['searchResultLayoutType'] == \"iaResult\":\n",
    "        # Filters need to be on separate lines because if the first condition is not satisfied, the `alias` key won't exist\n",
    "        # resulting in a KeyError upon code execution.\n",
    "        # if 'newark' in result['searchResultBusiness']['alias']: # SECOND FILTER TO BE USED\n",
    "            # Extra spaces added for cleaner print statement\n",
    "            print('name:        ', result['searchResultBusiness']['name'])\n",
    "\n",
    "            # to check if second filter works\n",
    "            print('alias:       ', result['searchResultBusiness']['alias']) \n",
    "            print('correct_city:', int('newark' in result['searchResultBusiness']['alias']))\n",
    "\n",
    "\n",
    "            print('review_count:', result['searchResultBusiness']['reviewCount'])\n",
    "            print('rating:      ', result['searchResultBusiness']['rating'], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f995770e",
   "metadata": {},
   "source": [
    "It appears that the filter is working as intended.\n",
    "\n",
    "Next, I want to write the results of my search inquiry into a `.csv` file so that I can read it into a Pandas DataFrame to manipulate it for analysis. I will be using the following format for the `.csv` file:\n",
    "\n",
    "`name`, `city`, `review_count`, `rating`\n",
    "\n",
    "Since my current `requests.get()` only gets the result of **one** search, I need to scrape the results of all the pages. Returning to Yelp, it appears that there is a search cap of $24$ pages. With $10$ restaurants in each page, this implies that there are $240$ restaurants in the city of Newark. The bay area Tri-Cities consists of Newark, Fremont, and Union City; this indicates there are $720$ search results. With my own persaonal understanding I know for a fact that these cities are not nearly big enough to have a cumulative total of $720$ restaurants, and this should be a reasonable assumption as I observed previously that not all results returned by Yelp are guaranteed to be in the city specified (hence the reason for the second filter). Nevertheless, I will perform searches on all $24$ pages per city and will rely on filters and later data cleaning (simply going to be dropping duplicates) to address data integrity problems.\n",
    "\n",
    "With all that in mind, I finally move on to writing a Python function to perform the scraping properly on my Yelp inquiries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a76dfb",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "The function `tri_city_extrapolate` is to scrape data from Yelp on the Bay Area Tri-Cities: Newark, Fremont, and Union City, while satisfying all the conditions determined during inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e8d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that scrapes the Yelp search results of the Bay Area Tri Cities: Newark, Fremont, and Union City.\n",
    "# The function will get the following information for each restaurant: 1) name, 2) city, 3) review_count, and\n",
    "# 4) rating. These elements together will represent a restaurant, and this restaurant information will be written\n",
    "# to a .csv file named \"tri_city_data.csv\".\n",
    "def tri_city_extrapolate():\n",
    "\n",
    "    # helper function to tri_city_extraoplate. Will take in a city name\n",
    "    def city_extrapolate(city):\n",
    "        for page_index in range(0, 231, 10): # 240 total, 0 is first ten, 230 is last ten\n",
    "            # String constructed by following the same URL format as a Yelp search\n",
    "            search_url = \"https://www.yelp.com/search/snippet?find_desc=Restaurants&find_loc=\" + city + \",+CA&start=\" + str(page_index) # API Endpoint\n",
    "            search_response = requests.get(search_url) # GET request to the API\n",
    "            search_results = search_response.json()['searchPageProps']['mainContentComponentsListProps'] # Index into the right granularity level\n",
    "            \n",
    "            # Loop through each result to write into file\n",
    "            for search_result in search_results:\n",
    "                # apply filters\n",
    "                if search_result['searchResultLayoutType'] == \"iaResult\":\n",
    "                    if 'newark' in search_result['searchResultBusiness']['alias']: \n",
    "                        # Create variables for each feature\n",
    "                        feature_name = search_result['searchResultBusiness']['name']\n",
    "                        feature_review_count = search_result['searchResultBusiness']['reviewCount']\n",
    "                        feature_rating = search_result['searchResultBusiness']['rating']\n",
    "                        # Concatenate features together to form tuple\n",
    "                        curr_tuple = feature_name + ',' + city + ',' + str(feature_review_count) + ',' + str(feature_rating) + '\\n'\n",
    "                        # Write tuple into destination file\n",
    "                        tri_city_data.write(curr_tuple)\n",
    "\n",
    "    tri_city_data = open(\"tri_city_data.csv\", \"w\") # Open the destination file\n",
    "    # Extrapolate results for tri-cities\n",
    "    for city in [\"Newark\", \"Fremont\", \"Union+City\"]: # Union City is formatted as \"Union+City\" in Yelp URL\n",
    "        city_extrapolate(city)\n",
    "    tri_city_data.close() # Close the destination file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1988dc",
   "metadata": {},
   "source": [
    "With `tri_city_extrapolate` implemented, I call the function in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d02fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data scraping on Yelp search inquiries\n",
    "tri_city_extrapolate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "339974a0",
   "metadata": {},
   "source": [
    "`tri_city_extrapolate` took about $1.5$ minutes to run; for $74$ page reads I think that's not bad. With the data scraping done, will move onto EDA and analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "566507ca",
   "metadata": {},
   "source": [
    "## Issues & Problems\n",
    "\n",
    "During this project, I ran into a few roadblocks that almost made me resort to manual data input. \n",
    "\n",
    "Before resorting to Yelp data scraping ([which I didn't know until midway thorugh this project that scraping is against Yelp Terms of Service](https://www.yelp-support.com/article/Can-I-copy-or-scrape-data-from-the-Yelp-site?l=en_US)), I first looked into the [Yelp Open Dataset](https://www.yelp.com/dataset). Since I seek to answer questions about the Bay Area Tri-Cities, I quickly came to notice that the dataset **did not** contain information on Newark, Fremont, or Union City. This meant that the Dataset was no longer applicable, thus I went back to scraping.\n",
    "\n",
    "I initially worked on this project using a Jupyter Notebook; for some reason, when performing my API GET call with `requests.get()`, I could not get the JSON response file's content in its entirety. I knew I wasn't getting the contents of the entire file because upon printing the last few characters of my API call, I could not see a closing brace; the end of JSON files are indicated with a closing brace. Google searches led me to a few Stack Overflow posts ([one](https://stackoverflow.com/questions/8049520/how-can-i-scrape-a-page-with-dynamic-content-created-by-javascript-in-python) and [two](https://stackoverflow.com/questions/56686756/python-requests-not-returning-fully-loaded-content)), which made me conclude that I was running into some Javascript rendering issue; how some contents of the page are not rendered unless the page is loaded. With only a little bit of experience in web design, I just assumed that I was out of luck.\n",
    "\n",
    "Finally, I decide to resort to using a web scraping software called [ParseHub](https://www.parsehub.com/). I was a bit disapointed that I had to use this tool as my initial motivations for starting this project was to learn more about web scraping. Nevertheless, I resigned myself to turning this project into a data exploration and analysis task. But upon finishing my download, I was confronted with this [page](https://help.parsehub.com/hc/en-us/articles/10607213112855-Using-ParseHub-in-macOS-2-20-Ventura); ParseHub is not supported on macOS 2.20 Ventura, which is the current OS I'm on. No, I am not going to set up a Virtual Machine just to use a tool that I didn't want to use in the first place.\n",
    "\n",
    "With all these shenanigans, I basically completely gave up on scraping and decided to manually input my data. To be fair, The Bay Area Tri-Cities are small, and I think I can handle manually inputting two-hundred or tuples.\n",
    "\n",
    "So I began my manual input, but before doing so, I shared my predicament with my dad. And to my pleasant surprise, my dad discovered that my code **worked** it ran perfectly fine on VS Code, but just **not on my Jupyter Notebook**. This made absolutely no sense to me. But hey, if it works, it works, so now here I am working on the rest of this project on VS Code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
